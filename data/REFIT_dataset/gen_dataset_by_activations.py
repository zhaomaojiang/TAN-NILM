import matplotlib.pyplot as plt
import numpy as np
from DataSet import DataLoader
import _pickle
from PreDefine import REFIT_building_info,REDD_building_info,UKDale_building_info,ap_name,data_name
from gen_activations import Tr_name, Te_name, Va_name, ratio  # ratio of the data is discarded

work_p = 0.8  # the probability of the appliance operating for each time period is 80%
sen_ratio = 0  # 0 means all real data is used, and 1 means that the ratio of real to synthetic is 1:1

if data_name == 'REDD':
    building_info = REDD_building_info
    batch_si = 32  # batch_size
    act_name = 'redd_act.pickle'  # Ackert here is then the name of the activation signal picker file
    # redd_bacthsize_appname_sam + sampling seconds_sen + the ratio of generated data to real data
    pickle_name = 'redd_'+str(batch_si)+ap_name[0:3]+'_sam3_sen1.pickle'
    all_ap = ['dishwasher', 'microwave', 'fridge']  # all appliances
elif data_name == 'UKDale':
    building_info = UKDale_building_info
    batch_si = 64  # batch_size
    act_name = 'ukdale_act.pickle'  # Ackert here is then the name of the activation signal picker file
    # pickle name is the name of the dictionary file generated by this program
    pickle_name = 'ukdale_' + str(batch_si)+ap_name[0:3]+'_sam6_sen0.pickle'
    all_ap = ['washingmachine', 'dishwasher',
              'microwave', 'fridge', 'kettle']  # all appliances
elif data_name == 'REFIT':
    building_info = REFIT_building_info
    batch_si = 64  # batch_size
    act_name = 'refit_act.pickle'  # Ackert here is then the name of the activation signal picker file
    # pickle name is the name of the dictionary file generated by this program
    pickle_name = 'refit_' + str(batch_si)+ap_name[0:3]+'_sam8_sen0.pickle'
    all_ap = ['washingmachine', 'dishwasher',
              'microwave', 'fridge', 'kettle']  # all appliances


def generator_step1(set_name, app_name, loader_name,loader_name1):
    """initial"""
    obj = DataLoader(file_location=data_name+'/'+set_name,
                     mixed_file_name=building_info[set_name]['total'],
                     appliance_file_name=building_info[set_name][app_name],
                     app_name=app_name)
    obj.dataloader_name = loader_name
    obj.dataloader_name1 = loader_name1
    print('Dataloader name: ', obj.dataloader_name)
    return obj


def generator_step2(obj):
    """Perform [File Reading], [Resampling], [Abnormal Point Removal] operations on the data object"""
    print('Read', obj.dataloader_name, 'origin data……')
    obj.mixed_data, obj.appliance_data = obj.read_origin_data(
        file_location=obj.file_location,
        mixed_file_name=obj.mixed_file_name,
        appliance_file_name=obj.appliance_file_name,
        separator=obj.separator,
        types=obj.types,
        # nrows=5000000  # test if you want
    )
    # resample, save the resampled data to set data, and assign the app name and mix name of the object
    print('Resampling', obj.dataloader_name, 'data……')
    obj.set_data, obj.mix_data_name, obj.app_data_name = obj.resample_data(
        index_name=obj.col_names[0],
        mixed_data=obj.mixed_data,
        appliance_data=obj.appliance_data,
        keep_origin=False
    )
    # todo: When debugging, if you want to display the signal, you can use the following code in the terminal
    # plt.figure()
    # plt.plot(obj.set_data['mix_data'])
    # plt.plot(obj.set_data['app_data'])
    # plt.title('before remove abnormal')
    # The resampled data is then followed by removal of outliers (upward spikes)
    print('', obj.dataloader_name, 'removing abnormal……')
    obj.set_data = obj.remove_abnormal_data(set_data=obj.set_data, num_remove=2)
    if obj.app_name == 'fridge':
        obj.set_data = obj.remove_big_abnormal()
    # todo: When debugging, if you want to display the signal, you can use the following code in the terminal
    # plt.figure()
    # plt.plot(obj.set_data[:, 0])
    # plt.plot(obj.set_data[:, 1])
    # plt.title('after remove abnormal')
    # plt.show()
    # Remove the first 10% of the data used to generate the activation signal
    obj.set_data = np.delete(obj.set_data, range(int(len(obj.set_data)*ratio)), axis = 0)
    return obj


def generator_step3(obj):
    """Perform the 'Generate Training Data by Batchnum Segmentation Raw Signal' operation on the data file"""
    if obj.dataloader_name == 'Train':
        ratio = sen_ratio  # The trained set generates data, and the tested and validated set will not generate data
    else:
        ratio = 0
    obj.dataset, obj.batch_num = obj.generate_dataset(data=obj.set_data, sythetic_ratio=ratio, work_probability=work_p)
    return obj


def generator_step4(obj):
    """The operation of slicing the data file into batches and combining the first and second types of signals"""
    obj.input_data = obj.generate_data_batch(batch_size=batch_si, all_ap_num=len(all_ap),
                                             data_set_name=obj.dataloader_name1)
    return obj


if __name__ == '__main__':
    # Initialize the Dataloader object
    Train = generator_step1(set_name=Tr_name, app_name=ap_name,
                            loader_name= 'Train', loader_name1='tr')
    Test = generator_step1(set_name=Te_name, app_name=ap_name,
                           loader_name= 'Test', loader_name1='te')
    Validation = generator_step1(set_name=Va_name, app_name=ap_name,
                                 loader_name= 'Validation', loader_name1='va')
    # Place the set of activation signals generated with Aktivations in each Dattalod object for easy access
    act = _pickle.load(open(act_name, 'rb'))
    Train.all_activation_set = act
    Test.all_activation_set = act
    Validation.all_activation_set = act

    # Perform step (1), data reading, resampling, and outlier removal
    Train = generator_step2(Train)
    Test = generator_step2(Test)
    Validation = generator_step2(Validation)
    MAX = [np.max(Train.set_data[:, 0]), np.max(Train.set_data[:, 1])]
    MAX_va = [np.max(Validation.set_data[:, 0]), np.max(Validation.set_data[:, 1])]

    # Proceed to step (2) to generate the dataset
    Train = generator_step3(Train)
    Test = generator_step3(Test)
    Validation = generator_step3(Validation)

    # Perform the data processing step (3) and the final step, generating the first and second type signals
    Train = generator_step4(Train)
    Test = generator_step4(Test)
    Validation = generator_step4(Validation)

    # Transfer useful data from objects, delete objects, and free up memory
    train_input = Train.input_data
    del Train
    test_input = Test.input_data
    del Test
    validation_input = Validation.input_data
    del Validation

    # Save the dictionary file
    save_dic = {'tr': train_input}
    del train_input
    save_dic['te'] = test_input
    del test_input
    save_dic['va'] = validation_input
    del validation_input
    _pickle.dump(save_dic, open(pickle_name, 'wb'))

    print('pickle_name: ', pickle_name, '!!!')
    del act_name, pickle_name, batch_si, work_p
    print('gen_dataset_by_activations: all load over')
